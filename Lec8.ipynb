{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kterashi/niigata_lecture_2024/blob/master/Lec8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Quantum Applications to Charged Particle Tracking"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tracking with Annealing Technique\n",
    "\n",
    "In this exercise, we try to find tracks from detector hits using simulated annealing technique. This is based on the <a href=\"https://github.com/derlin/hepqpr-qallse\" target=\"_blank\">hepqpr-qallse</a> framework developed by Lucy Linder and LBNL group.\n",
    "\n",
    "First, additional modules are installed. \n",
    "\n",
    "**Note that you will likely get warning and error message about restarting the session due to the dependency of pyparsing package. Please just push \"セッションを再起動する (RESTART SESSION)\", and move on to the next cell. This should be sufficient.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install dwave-qbsolv dwave-neal 'git+https://github.com/LAL/trackml-library.git'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, let us import necessary modules as usual."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# First, get all the necessary libraries from the copy and import packages\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import tarfile\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "shutil.copy('/content/gdrive/MyDrive/qcintro.tar.gz', '.')\n",
    "with tarfile.open('qcintro.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall(path='/root/.local')\n",
    "\n",
    "sys.path.append('/root/.local/lib/python3.10/site-packages')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2024-12-07T08:09:01.643474Z",
     "start_time": "2024-12-07T08:08:03.565823Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tested with python 3.10.11, qiskit 0.42.1, numpy 1.23.5, scipy 1.9.3\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "from qiskit_aer import AerSimulator\n",
    "from qiskit.circuit.library import TwoLocal\n",
    "from qiskit.primitives import Estimator, Sampler, BackendEstimator\n",
    "from qiskit.quantum_info import SparsePauliOp, Statevector\n",
    "from qiskit_algorithms.minimum_eigensolvers import VQE, NumPyMinimumEigensolver\n",
    "from qiskit_algorithms.gradients import ParamShiftEstimatorGradient\n",
    "from qiskit_algorithms.optimizers import SPSA, COBYLA\n",
    "from qiskit_optimization.applications import OptimizationApplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next, create a dataset of detector hits. The parameter `density` controls how many particles in an event are included in the dataset. The density = 0.0015 means the particle density corresponds to about 0.15% of a typical HL-LHC collision event.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from hepqpr.qallse.dsmaker import create_dataset\n",
    "\n",
    "density = 0.0015\n",
    "\n",
    "output_path = os.getcwd()+'/ds'\n",
    "prefix = 'ds'+str(density)\n",
    "\n",
    "metadata, path = create_dataset(\n",
    "    density=density,\n",
    "    output_path=output_path,\n",
    "    prefix=prefix,\n",
    "    random_seed=17322,  # density=0.0015, 17 segments \n",
    "    #random_seed=11029,  # density=0.0015, 26 segments\n",
    "    #random_seed=17310,  # density=0.0015, 25 segments\n",
    "    gen_doublets=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "You will see that the dataset is created under ds in your working directory.\n",
    "\n",
    "Next, QUBO is produced from the dataset by reconstructing doublets, triplets and quadruplets from the hits and checking their relative geometries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hepqpr.qallse import *\n",
    "\n",
    "# ==== BUILD CONFIG\n",
    "loglevel = logging.INFO\n",
    "\n",
    "input_path = os.getcwd()+'/ds/'+prefix+'/event000001000-hits.csv'\n",
    "output_path = os.getcwd()+'/ds/'+prefix+'/'\n",
    "\n",
    "model_class = QallseD0  # model class to use\n",
    "extra_config = dict()  # model config\n",
    "\n",
    "dump_config = dict(\n",
    "    output_path = os.getcwd()+'/ds/'+prefix+'/',\n",
    "    prefix=prefix+'_',\n",
    "    xplets_kwargs=dict(format='json', indent=3), # use json (vs \"pickle\") and indent the output\n",
    "    qubo_kwargs=dict(w_marker=None, c_marker=None) # save the real coefficients VS generic placeholders\n",
    ")\n",
    "\n",
    "# ==== configure logging\n",
    "logging.basicConfig(\n",
    "    stream=sys.stderr,\n",
    "    format=\"%(asctime)s.%(msecs)03d [%(name)-15s %(levelname)-5s] %(message)s\",\n",
    "    datefmt='%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "logging.getLogger('hepqpr').setLevel(loglevel)\n",
    "\n",
    "# ==== build model\n",
    "# load data\n",
    "dw = DataWrapper.from_path(input_path)\n",
    "doublets = pd.read_csv(input_path.replace('-hits.csv', '-doublets.csv'))\n",
    "\n",
    "# build model\n",
    "model = model_class(dw, **extra_config)\n",
    "model.build_model(doublets)\n",
    "\n",
    "# dump model to a file\n",
    "dumper.dump_model(model, **dump_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Set up the annealing job by loading QUBO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-07T08:10:08.435003Z",
     "start_time": "2024-12-07T08:10:08.379201Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os.path import join as path_join\n",
    "\n",
    "from hepqpr.qallse.other.stdout_redirect import capture_stdout\n",
    "from hepqpr.qallse.other.dw_timing_recorder import solver_with_timing, TimingRecord\n",
    "from hepqpr.qallse.plotting import *\n",
    "\n",
    "\n",
    "# ==== RUN CONFIG\n",
    "nreads = 10\n",
    "nseed = 1000000\n",
    "\n",
    "loglevel = logging.INFO\n",
    "\n",
    "input_path = os.getcwd()+'/ds/'+prefix+'/event000001000-hits.csv'\n",
    "qubo_path = os.getcwd()+'/ds/'+prefix+'/'\n",
    "\n",
    "# ==== configure logging\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout,\n",
    "    format=\"%(asctime)s.%(msecs)03d [%(name)-15s %(levelname)-5s] %(message)s\",\n",
    "    datefmt='%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "logging.getLogger('hepqpr').setLevel(loglevel)\n",
    "\n",
    "# ==== build model\n",
    "# load data\n",
    "dw = DataWrapper.from_path(input_path)\n",
    "pickle_file = prefix+'_qubo.pickle'\n",
    "with open(path_join(qubo_path, pickle_file), 'rb') as f:\n",
    "    Q = pickle.load(f)\n",
    "#print(Q)\n",
    "\n",
    "import time\n",
    "start_time = time.process_time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Perform simulated annealing using neal software.\n",
    "\n",
    "If it's successfully done, you can see a file \"plot_ds..._tracks_found.html\" in created in your working directory. This plot displays the detector hits in QUBO, projected onto a plane perpendicular to the beam axis, and shows which detector hits are successfully selected to from reconstructed tracks. The green lines correspond to reconstructed tracks, the blue lines missing tracks (i.e, tracks that were not reconstructed) and the red ones fake tracks (i.e, misreconstructed tracks that do not mach truth particles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample qubo\n",
    "\n",
    "# --- neal\n",
    "import neal\n",
    "sampler = neal.SimulatedAnnealingSampler()\n",
    "#import dimod\n",
    "#sampler = dimod.RandomSampler()\n",
    "response = sampler.sample_qubo(Q, num_reads=nreads, seed=nseed)\n",
    "\n",
    "exec_time = time.process_time() - start_time\n",
    "print(f'QUBO of size {len(Q)} sampled in {exec_time:.2f}s (NEAL).')\n",
    "print('')\n",
    "\n",
    "\n",
    "# get the results\n",
    "all_doublets = Qallse.process_sample(next(response.samples()))\n",
    "final_tracks, final_doublets = TrackRecreaterD().process_results(all_doublets)\n",
    "\n",
    "# compute stats\n",
    "en0 = dw.compute_energy(Q)\n",
    "en = response.record.energy[0]\n",
    "occs = response.record.num_occurrences\n",
    "\n",
    "p, r, ms = dw.compute_score(final_doublets)\n",
    "trackml_score = dw.compute_trackml_score(final_tracks)\n",
    "\n",
    "# print stats\n",
    "print(f'SAMPLE -- energy: {en:.4f}, ideal: {en0:.4f} (diff: {en-en0:.6f})')\n",
    "print(f'          best sample occurrence: {occs[0]}/{occs.sum()}')\n",
    "\n",
    "print(f'SCORE  -- precision (%): {p * 100}, recall (%): {r * 100}, missing: {len(ms)}')\n",
    "print(f'          tracks found: {len(final_tracks)}, trackml score (%): {trackml_score * 100}')\n",
    "\n",
    "# plotting examples\n",
    "dims = ['x', 'y']\n",
    "dout = 'plot_'+prefix+'_tracks_found.html'\n",
    "iplot_results(dw, final_doublets, ms, dims=dims, filename=dout)\n",
    "#iplot_results_tracks(dw, final_tracks)\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(dout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Attention\n",
    "**Below we try to reconstruct tracks with VQE in a quantum circuit model. Since each segment is assigned to a qubit in this (naive) approach, the memory consumption quickly becomes explosive with the number of segments and the kernel will crash if the number of segments is larger than about 30 or so. Before proceeding, please make sure that the number of segments has to be less than 20-25.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## VQE Technique with Energy Minimization\n",
    "\n",
    "Here, we discuss techniques to use VQE for charged particle tracking. In order to use VQE, the problem will need to be formulated in the form of Hamiltonian. If the problem is formulated such that the solution corresponds to the lowest energy state of the Hamiltonian, the VQE could solve the problem by finding such state."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### QUBO Format\n",
    "\n",
    "Under this setup, the next step is whether a given segment is adopted as part of particle tracks or rejected as fake. In a sample of $N$ segments, the adoptation or rejection of $i$-th segment is associated to 1 or 0 of a binary variable $T_i$, and the variable $T_i$ is determined such that the objective function defined as\n",
    "\n",
    "$$\n",
    "O(b, T) = \\sum_{i=1}^N a_{i} T_i + \\sum_{i=1}^N \\sum_{j<i}^N b_{ij} T_i T_j\n",
    "$$\n",
    "\n",
    "is minimized. Here $a_i$ is the score of $i$-th segment and $b_{ij}$ is the score of the pair of $i$- and $j$-th segments. The objective function becomes smaller by selecting segments that have smaller $a_i$ values (pointing towards the detector center) and are paired with other segments with smaller $b_{ij}$ values (more consistent with a real track) and rejecting otherwise. Once the correct segments are identified, the corresponding tracks can be reconstructed with high efficiency. Therefore, solving this minimization problem is the key to tracking.\n",
    "\n",
    "Let us first extract the scores $a_i$ and $b_{ij}$ from the QUBO produced above (corresponding to the variable Q).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_max = 100\n",
    "\n",
    "nvar = 0\n",
    "key_i = []\n",
    "a_score = np.zeros(n_max)\n",
    "for (k1, k2), v in Q.items():\n",
    "    if k1 == k2:\n",
    "        a_score[nvar] = v\n",
    "        key_i.append(k1)\n",
    "        nvar += 1\n",
    "a_score = a_score[:nvar]\n",
    "\n",
    "b_score = np.zeros((n_max,n_max))\n",
    "for (k1, k2), v in Q.items():\n",
    "    if k1 != k2:\n",
    "        for i in range(nvar):\n",
    "            for j in range(nvar):\n",
    "                if k1 == key_i[i] and k2 == key_i[j]:\n",
    "                    if i < j:\n",
    "                        b_score[j][i] = v\n",
    "                    else:\n",
    "                        b_score[i][j] = v\n",
    "\n",
    "b_score = b_score[:nvar,:nvar]\n",
    "\n",
    "print(f'# of Segments: {nvar}')\n",
    "# Print out the first 5x5\n",
    "print(a_score[:5])\n",
    "print(b_score[:5, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Ising Format\n",
    "\n",
    "The QUBO objective function is not the form of Hamiltonian (i.e, not Hermitian operator). Therefore, the objective function needs to be transformed before solving with VQE. Given that $T_i$ takes a binary value $\\{0, 1\\}$, a new variable $s_i$ with values of $\\{+1, -1\\}$ can be defined by\n",
    "\n",
    "$$\n",
    "T_i = \\frac{1}{2} (1 - s_i).\n",
    "$$\n",
    "\n",
    "Note that $\\{+1, -1\\}$ is the eigenvalue of Pauli operator. By replacing $s_i$ with Pauli $Z$ operator acting on $i$-th qubit, we can obtain the following Hamiltonian for which the computational basis states in $N$-qubit system correspond to the eigenstates that encode adoptation or rejection of the segments:\n",
    "\n",
    "$$\n",
    "H(h, J, s) = \\sum_{i=1}^N h_i Z_i + \\sum_{i=1}^N \\sum_{j<i}^N J_{ij} Z_i Z_j + \\text{(constant)}\n",
    "$$\n",
    "\n",
    "The form of this Hamiltonian is the same as Ising model Hamiltonian, which often appears in various fields of natural science. The $\\text{constant}$ is a constant term and has no impact in variational method, hence ignored in the rest of this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exercise\n",
    "\n",
    "By following the above prescription, please calculate the coefficients $h_i$ and $J_{ij}$ of the Hamiltonian in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-07T08:10:23.680082Z",
     "start_time": "2024-12-07T08:10:23.671275Z"
    }
   },
   "outputs": [],
   "source": [
    "num_qubits = nvar\n",
    "\n",
    "coeff_h = np.zeros(num_qubits)\n",
    "coeff_J = np.zeros((num_qubits, num_qubits))\n",
    "\n",
    "# Calculate coeff_h and coeff_J from a_score and b_score\n",
    "coeff_h = -(a_score / 2. + (np.sum(b_score, axis=0) + np.sum(b_score, axis=1)) / 4.)\n",
    "coeff_J = b_score / 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next, let us define the Hamiltonian used in VQE as a SparsePauliOp object. In VQE exercise of Hands-on Exercise (3), the SparsePauliOp was used to define a single Pauli string $ZXY$, but the same class can be used for the sum of Pauli strings. For example,\n",
    "\n",
    "$$\n",
    "H = 0.2 IIZ + 0.3 ZZI + 0.1 ZIZ\n",
    "$$\n",
    "\n",
    "can be expressed as\n",
    "\n",
    "```python\n",
    "H = SparsePauliOp(['IIZ', 'ZZI', 'ZIZ'], coeffs=[0.2, 0.3, 0.1])\n",
    "```\n",
    "\n",
    "Note that the qubits are ordered from right to left (the most right operator acts on the 0-th qubit) according to the rule in Qiskit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Pick up all the Pauli strings with non-zero coefficients and make the array of corresponding coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-07T08:10:33.121097Z",
     "start_time": "2024-12-07T08:10:33.096859Z"
    }
   },
   "outputs": [],
   "source": [
    "pauli_products = []\n",
    "coeffs = []\n",
    "\n",
    "for iq in range(num_qubits):\n",
    "    if np.isclose(coeff_h[iq], 0.):\n",
    "        continue\n",
    "\n",
    "    pauli_products.append(('I' * (num_qubits - iq - 1)) + 'Z' + ('I' * iq))\n",
    "    coeffs.append(coeff_h[iq])\n",
    "\n",
    "for iq in range(num_qubits):\n",
    "    for jq in range(iq):\n",
    "        if np.isclose(coeff_J[iq, jq], 0.):\n",
    "            continue\n",
    "\n",
    "        pauli = 'I' * (num_qubits - iq - 1)\n",
    "        pauli += 'Z'\n",
    "        pauli += 'I' * (iq - jq - 1)\n",
    "        pauli += 'Z'\n",
    "        pauli += 'I' * jq\n",
    "        pauli_products.append(pauli)\n",
    "\n",
    "        coeffs.append(coeff_J[iq, jq])\n",
    "\n",
    "\n",
    "hamiltonian = SparsePauliOp(pauli_products, coeffs=coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Executing VQE\n",
    "\n",
    "Now we try to approximately obtain the lowest energy eigenvalues using VQE with the Hamiltonian defined above. But, before doing that, let us diagonalize the Hamiltonian matrix and calculate the exact energy eigenvalues and eigenstates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Diagonalize the Hamiltonian and calculate the energy eigenvalues and eigenstates\n",
    "ee = NumPyMinimumEigensolver()\n",
    "result_diag = ee.compute_minimum_eigenvalue(hamiltonian)\n",
    "\n",
    "# Print out the combination of qubits corresponding to the lowest energy\n",
    "print(f'Minimum eigenvalue (diagonalization): {result_diag.eigenvalue.real}')\n",
    "# Expand the state with computational bases and select the one with the highest probability\n",
    "optimal_segments_diag = OptimizationApplication.sample_most_likely(result_diag.eigenstate)\n",
    "print(f'Optimal segments (diagonalization): {optimal_segments_diag}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we move to VQE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "backend = AerSimulator()\n",
    "# Create Estimator instance\n",
    "estimator = BackendEstimator(backend)\n",
    "\n",
    "# Define variational form of VQE using a built-in function called TwoLocal.\n",
    "ansatz = TwoLocal(num_qubits, 'ry', 'cz', 'linear', reps=1)\n",
    "\n",
    "# Optimizer\n",
    "optimizer_name = 'SPSA'\n",
    "\n",
    "if optimizer_name == 'SPSA':\n",
    "    optimizer = SPSA(maxiter=300)\n",
    "    grad = ParamShiftEstimatorGradient(estimator)\n",
    "\n",
    "elif optimizer_name == 'COBYLA':\n",
    "    optimizer = COBYLA(maxiter=500)\n",
    "    grad = None\n",
    "\n",
    "# Initialize parameters with random values\n",
    "rng = np.random.default_rng()\n",
    "init = rng.uniform(0., 2. * np.pi, size=len(ansatz.parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make VQE object and search for the ground state\n",
    "vqe = VQE(estimator, ansatz, optimizer, gradient=grad, initial_point=init)\n",
    "result_vqe = vqe.compute_minimum_eigenvalue(hamiltonian)\n",
    "\n",
    "# Create state vector from the ansatz using optimized parameters\n",
    "optimal_state = Statevector(ansatz.assign_parameters(result_vqe.optimal_parameters))\n",
    "\n",
    "# Print out the combination of qubits with the lowest energy\n",
    "print(f'Minimum eigenvalue (VQE): {result_vqe.eigenvalue.real}')\n",
    "optimal_segments_vqe = OptimizationApplication.sample_most_likely(optimal_state)\n",
    "print(f'Optimal segments (VQE): {optimal_segments_vqe}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Check again reconstructed tracks in the detector plane for fun. You can switch the results of exact diagonalization and VQE with the variable `type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hepqpr.qallse import DataWrapper, Qallse, TrackRecreaterD\n",
    "from hepqpr.qallse.plotting import iplot_results, iplot_results_tracks\n",
    "from hepqpr.qallse.utils import diff_rows\n",
    "\n",
    "# Results tp show: diag = Exact diagonalization, vqe = VQE\n",
    "type = \"diag\"\n",
    "#type = \"vqe\"\n",
    "\n",
    "if type == \"diag\":\n",
    "    optimal_segments = optimal_segments_diag\n",
    "elif type == \"vqe\":\n",
    "    optimal_segments = optimal_segments_vqe\n",
    "\n",
    "samples = dict(zip(key_i, optimal_segments))\n",
    "\n",
    "# get the results\n",
    "all_doublets = Qallse.process_sample(samples)\n",
    "\n",
    "final_tracks, final_doublets = TrackRecreaterD().process_results(all_doublets)\n",
    "\n",
    "#dw = DataWrapper.from_path('data/event000001000-hits.csv')\n",
    "input_path = os.getcwd()+'/ds/'+prefix+'/event000001000-hits.csv'\n",
    "dw = DataWrapper.from_path(input_path)\n",
    "\n",
    "p, r, ms = dw.compute_score(final_doublets)\n",
    "trackml_score = dw.compute_trackml_score(final_tracks)\n",
    "\n",
    "print(f'SCORE  -- precision (%): {p * 100}, recall (%): {r * 100}, missing: {len(ms)}')\n",
    "print(f'          tracks found: {len(final_tracks)}, trackml score (%): {trackml_score * 100}')\n",
    "\n",
    "dims = ['x', 'y']\n",
    "_, missings, _ = diff_rows(final_doublets, dw.get_real_doublets())\n",
    "dout = 'plot-ising_'+type+'_found_tracks.html'\n",
    "iplot_results(dw, final_doublets, missings, dims=dims, filename=dout)\n",
    "\n",
    "HTML(dout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
